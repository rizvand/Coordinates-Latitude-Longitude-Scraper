{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latitude & Longitude Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intall dependencies**\n",
    "\n",
    "Install selenium terlebih dahulu dengan `!pip install selenium`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this code to install selenium\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run cell di bawah ini untuk import beberapa library yang akan digunakan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T07:44:08.669433Z",
     "start_time": "2020-10-16T07:44:08.660433Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from tqdm import tqdm_notebook as tqdmn\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**\n",
    "\n",
    "1. Pastikan chromedriver berada dalam satu folder dengan file notebook ini, jika tidak maka isi variabel `exec_path` pada baris kedua kode di bawah ini dengan filepath yang sesuai dengan letak file `chromedriver.exe`\n",
    "2. Pastikan chrome versi 86.0.4240.75, jika tidak bisa update chrome dahulu agar compatible dengan chromedrivernya\n",
    "3. Jika sudah sesuai maka run code di bawah ini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T07:44:09.408436Z",
     "start_time": "2020-10-16T07:44:09.401414Z"
    }
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "\n",
    "#Batas bawah dan batas atas waktu tunggu per iterasi\n",
    "#Contoh: delay per iterasi 2-5 detik, maka set delay_lower_bound = 2 dan delay_upper_bound = 5\n",
    "delay_lower_bound = 2\n",
    "delay_upper_bound = 5\n",
    "\n",
    "#Tambahan waktu (detik) untuk scraper mengekstrak koordinat\n",
    "implicit_wait = 1\n",
    "\n",
    "#Jumlah row per partisi\n",
    "part_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T07:44:14.147401Z",
     "start_time": "2020-10-16T07:44:14.101414Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_coordinates(file, sheetname, address_col, output):\n",
    "    exec_path = 'chromedriver.exe'\n",
    "    #Read input file\n",
    "    try:\n",
    "        if file.rsplit('.', 1)[-1] == 'xlsx':\n",
    "            df_all = pd.read_excel(file, sheet_name= sheetname)\n",
    "        elif file.rsplit('.', 1)[-1] == 'csv':\n",
    "            df_all = pd.read_csv(file)\n",
    "\n",
    "    except:\n",
    "        raise Exception(\"File input error\")\n",
    "        \n",
    "    driver = webdriver.Chrome(exec_path)\n",
    "    \n",
    "    #Partition\n",
    "    n_part = int(np.floor(df_all.shape[0]/part_size) + 1)\n",
    "    df_result = pd.DataFrame()\n",
    "    \n",
    "    for i in range(n_part):\n",
    "        df = df_all.copy()\n",
    "        if i == n_part:\n",
    "            df = df_all.copy().iloc[i*part_size:df_all.shape[0], :]\n",
    "        else:\n",
    "            df = df_all.copy().iloc[(i*part_size):(part_size*(i+1)), :]\n",
    "    \n",
    "        #Scraping coordinates\n",
    "        url_with_coordinates = []\n",
    "        lat_scrape = []\n",
    "        long_scrape = []\n",
    "        colnames = address_col\n",
    "        for item in tqdmn(df[colnames]):\n",
    "            url = 'https://www.google.com/maps/search/' + str(item)\n",
    "            delay = np.random.rand()*(delay_upper_bound - delay_lower_bound) + delay_lower_bound\n",
    "            time.sleep(delay)\n",
    "            driver.get(url)\n",
    "            driver.implicitly_wait(implicit_wait)\n",
    "            url_with_coordinates.append(driver.find_element_by_css_selector('meta[itemprop=image]').get_attribute('content'))\n",
    "        #driver.close()\n",
    "\n",
    "        df['url_with_coordinates'] = url_with_coordinates\n",
    "        for url in df['url_with_coordinates']:\n",
    "            try:\n",
    "                lat_scrape.append(url.split('?center=')[1].split('&zoom=')[0].split('%2C')[0])\n",
    "            except:\n",
    "                lat_scrape.append('NA')\n",
    "            try:\n",
    "                long_scrape.append(url.split('?center=')[1].split('&zoom=')[0].split('%2C')[1])\n",
    "            except:\n",
    "                long_scrape.append('NA')\n",
    "\n",
    "        df['lat_scrape'] = lat_scrape\n",
    "        df['long_scrape'] = long_scrape\n",
    "        \n",
    "        #Export hasil ke dalam excel bernama 'HasilScraping.xlsx'\n",
    "        df_result = pd.concat([df_result, df], axis = 0)\n",
    "        df_result.to_excel(output, index = False)\n",
    "        \n",
    "        #print\n",
    "        print('Partisi ke-{} dari {} selesai'.format(i+1, n_part))\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Menjalankan Fungsi**\n",
    "1. Untuk melakukan scraping, dapat dilakukan dengan menjalankan fungsi `scrape_coordinates` dengan 3 argumen, yaitu file, sheetname, dan address_col\n",
    "2. `file` diisi dengan lokasi input file alamat yang ingin dilakukan scraping. Untuk memudahkan, letakkan file input dalam satu folder dengan notebook ini. Pastikan file input berekstensi `.xlsx` atau `.csv`, jika tidak maka akan menghasilkan error.\n",
    "3. `sheetname` merupakan nama sheet pada excel yang berisi informasi alamat yang ingin discraping\n",
    "4. `address_col` merupakan nama kolom yang berisi alamat-alamat yang ingin discraping\n",
    "5. `output` merupakan nama file hasil export (excel)\n",
    "\n",
    "Misalkan alamat yang ingin discrape berada pada file bernama `dummy1.xlsx` pada `Sheet1` dan pada kolom `Address` serta hasil akan diexport ke dalam excel bernama `HasilScraping.xlsx`, maka jalankan fungsi `scrape_coordinates(file = 'dummy1.xlsx', sheetname = 'Sheet1', address_col = 'Address', output = 'HasilScraping.xlsx')`\n",
    "\n",
    "Hasil scraping akan diinput pada kolom `lat_scrape` dan `long_scrape` url hasil scraping yang mengandung informasi koordinat diinput ke dalam kolom `url_with_coordinates` (kolom ini dapat dihapus jika memang tidak diperlukan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T08:04:27.616990Z",
     "start_time": "2020-10-16T07:44:21.219432Z"
    }
   },
   "outputs": [],
   "source": [
    "#Contoh scraping dan memasukkannya pada variabel df_scrape serta hasil di save pada excel 'HasilScraping.xlsx'\n",
    "df_scrape = scrape_coordinates(file = 'dummy1.xlsx', \n",
    "                               sheetname = 'Sheet1', \n",
    "                               address_col = 'Address', \n",
    "                               output = 'HasilScraping.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Koordinat yang tidak bisa ditemukan**\n",
    "\n",
    "Untuk koordinat yang tidak bisa diperoleh melalui scraping akan ditandai dengan `'NA'`. Dalam beberapa kasus, koordinat tidak bisa ditemukan salah satunya karena alamat yang diberikan tidak bisa ditemukan melalui google maps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
